{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Build your own NLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "Data cleaning / processing / language parsing\n",
    "Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "Assess your models using cross-validation and determine whether one model performed better.\n",
    "Pick one of the models and try to increase accuracy by at least 5 percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import state_union, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\state_union.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('state_union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama = open(\"obama2016.txt\")\n",
    "trump = open(\"trump2019.txt\")\n",
    "obama = obama.read()\n",
    "trump = trump.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse using SpaCy\n",
    "nlp = spacy.load('en')\n",
    "obama_doc = nlp(obama)\n",
    "trump_doc = nlp(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(February, 5, ,, 2019, \\n, 9:07, P.M., EST, \\n\\n)</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(THE, PRESIDENT, :,  , Madam, Speaker, ,, Mr.,...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(—, and, my, fellow, Americans, :, \\n\\n)</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(We, meet, tonight, at, a, moment, of, unlimit...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(As, we, begin, a, new, Congress, ,, I, stand,...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0      1\n",
       "0  (February, 5, ,, 2019, \\n, 9:07, P.M., EST, \\n\\n)  Trump\n",
       "1  (THE, PRESIDENT, :,  , Madam, Speaker, ,, Mr.,...  Trump\n",
       "2           (—, and, my, fellow, Americans, :, \\n\\n)  Trump\n",
       "3  (We, meet, tonight, at, a, moment, of, unlimit...  Trump\n",
       "4  (As, we, begin, a, new, Congress, ,, I, stand,...  Trump"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences\n",
    "trump_sents = [[sent, 'Trump'] for sent in trump_doc.sents]\n",
    "obama_sents = [[sent, 'Obama'] for sent in obama_doc.sents]\n",
    "\n",
    "# Combine\n",
    "sentences = pd.DataFrame(trump_sents + obama_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "February 5, 2019\n",
      "9:07 P.M. EST\n",
      "\n",
      "THE PRESIDENT:  Madam Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States — (applause) — and my fellow Americans:\n",
      "\n",
      "We meet tonight at a moment of unlimited potential.  As we begin a new Congress, I stand here ready to work with you to achieve historic breakthroughs for all Americans.\n",
      "\n",
      "Millions of our fellow citizens are watching us now, gathered in this great chamber, hoping that we will govern not as\n",
      "\n",
      "Trump speech length: 7498\n",
      "\n",
      " 9:10 P.M. EST\n",
      "\n",
      "Mr. Speaker, Mr. Vice President, Members of Congress, my fellow Americans:\n",
      "\n",
      "Tonight marks the eighth year that I’ve come here to report on the State of the Union. And for this final one, I’m going to try to make it a little shorter. (Applause.) I know some of you are antsy to get back to Iowa. (Laughter.) I've been there. I'll be shaking hands afterwards if you want some tips. (Laughter.)\n",
      "\n",
      "\n",
      "\n",
      "Obama speech length: 7473\n"
     ]
    }
   ],
   "source": [
    "# Look at excerpts from each \n",
    "print(trump_doc[:100])\n",
    "print('\\nTrump speech length:', len(trump_doc))\n",
    "\n",
    "print('\\n', obama_doc[:100])\n",
    "print('\\nObama speech length:', len(obama_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words function for each text\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # filter out punctuation and stop words\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return most common words\n",
    "    return [item[0] for item in Counter(allwords).most_common(500)]\n",
    "\n",
    "# Get bags \n",
    "trump_words = bag_of_words(trump_doc)\n",
    "obama_words = bag_of_words(obama_doc)\n",
    "\n",
    "# Combine bags to create common set of unique words\n",
    "common_words = set(trump_words + obama_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words data frame using combined common words and sentences\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Build data frame\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentences in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentences\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imperative</th>\n",
       "      <th>city</th>\n",
       "      <th>moment</th>\n",
       "      <th>focus</th>\n",
       "      <th>moral</th>\n",
       "      <th>hand</th>\n",
       "      <th>ask</th>\n",
       "      <th>story</th>\n",
       "      <th>modern</th>\n",
       "      <th>work</th>\n",
       "      <th>...</th>\n",
       "      <th>take</th>\n",
       "      <th>entrepreneur</th>\n",
       "      <th>want</th>\n",
       "      <th>question</th>\n",
       "      <th>this</th>\n",
       "      <th>find</th>\n",
       "      <th>intend</th>\n",
       "      <th>infrastructure</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(February, 5, ,, 2019, \\n, 9:07, P.M., EST, \\n\\n)</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(THE, PRESIDENT, :,  , Madam, Speaker, ,, Mr.,...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(—, and, my, fellow, Americans, :, \\n\\n)</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(We, meet, tonight, at, a, moment, of, unlimit...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(As, we, begin, a, new, Congress, ,, I, stand,...</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  imperative city moment focus moral hand ask story modern work     ...      \\\n",
       "0          0    0      0     0     0    0   0     0      0    0     ...       \n",
       "1          0    0      0     0     0    0   0     0      0    0     ...       \n",
       "2          0    0      0     0     0    0   0     0      0    0     ...       \n",
       "3          0    0      1     0     0    0   0     0      0    0     ...       \n",
       "4          0    0      0     0     0    0   0     0      0    1     ...       \n",
       "\n",
       "  take entrepreneur want question this find intend infrastructure  \\\n",
       "0    0            0    0        0    0    0      0              0   \n",
       "1    0            0    0        0    0    0      0              0   \n",
       "2    0            0    0        0    0    0      0              0   \n",
       "3    0            0    0        0    0    0      0              0   \n",
       "4    0            0    0        0    0    0      0              0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (February, 5, ,, 2019, \\n, 9:07, P.M., EST, \\n\\n)       Trump  \n",
       "1  (THE, PRESIDENT, :,  , Madam, Speaker, ,, Mr.,...       Trump  \n",
       "2           (—, and, my, fellow, Americans, :, \\n\\n)       Trump  \n",
       "3  (We, meet, tonight, at, a, moment, of, unlimit...       Trump  \n",
       "4  (As, we, begin, a, new, Congress, ,, I, stand,...       Trump  \n",
       "\n",
       "[5 rows x 777 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create bow features \n",
    "bow = bow_features(sentences, common_words)\n",
    "bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab sentence level documents in NLTK\n",
    "obama = open('obama2016.txt')\n",
    "trump = open('trump2019.txt')\n",
    "obama = obama.read()\n",
    "trump = trump.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(sent):\n",
    "    doc_info = []\n",
    "    i = 0\n",
    "    for sent in text_sents_clean:\n",
    "        i += 1\n",
    "        count = count_words(sents)\n",
    "        temp = {'doc_id' : i, 'doc_lenth': count}\n",
    "        doc_info.append(temp)\n",
    "    return doc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of text \n",
    "obama_list = [\" \".join(sents) for sents in obama]\n",
    "trump_list = [\" \".join(sents) for sents in trump]\n",
    "joined = obama_list + trump_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-a69742255366>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                             )\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoined\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1611\u001b[0m         \"\"\"\n\u001b[0;32m   1612\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1613\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1614\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1615\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1031\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    960\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    963\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# Vectorize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                             min_df=2, \n",
    "                             stop_words='english',   \n",
    "                             use_idf=True,\n",
    "                             norm=u'l2', \n",
    "                             smooth_idf=True \n",
    "                            )\n",
    "\n",
    "tfidf = vectorizer.fit_transform(joined).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is a list of the stopwords identified by NLTK.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# All the processing work is done here, so it may take a while.\n",
    "obama_doc = nlp(obama)\n",
    "trump_doc = nlp(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The obama_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 7473 tokens long\n",
      "The first three tokens are '9:10 P.M. EST'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the objects we've built.\n",
    "print(\"The obama_doc object is a {} object.\".format(type(obama_doc)))\n",
    "print(\"It is {} tokens long\".format(len(obama_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(obama_doc[:3]))\n",
    "print(\"The type of each token is {}\".format(type(obama_doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trump_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 7498 tokens long\n",
      "The first three tokens are 'February 5,'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the objects we've built.\n",
    "print(\"The trump_doc object is a {} object.\".format(type(trump_doc)))\n",
    "print(\"It is {} tokens long\".format(len(trump_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(trump_doc[:3]))\n",
    "print(\"The type of each token is {}\".format(type(trump_doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama: [('the', 274), ('to', 206), ('of', 149), ('and', 142), ('that', 130), ('we', 121), ('a', 117), ('in', 99), ('\\n\\n', 86), ('our', 85), ('Applause', 84), ('is', 73), ('I', 69), ('’s', 68), ('for', 54), ('it', 53), ('And', 51), ('or', 51), ('who', 41), ('not', 41), ('on', 40), ('It', 37), ('have', 37), ('this', 36), ('do', 35), ('n’t', 34), ('us', 34), ('America', 33), ('as', 33), ('are', 31), ('We', 31), ('you', 30), ('they', 28), ('But', 26), ('people', 26), ('That', 26), ('their', 26), ('be', 25), ('will', 25), ('just', 24), ('world', 23), ('work', 22), ('more', 22), ('with', 22), (\"'s\", 22), ('American', 22), ('’ve', 21), ('can', 20), ('all', 20), ('make', 19), ('if', 19), ('want', 19), ('when', 19), ('by', 19), ('should', 19), ('up', 18), ('years', 18), ('change', 18), ('new', 18), ('year', 17), ('there', 17), ('because', 17), ('even', 17), ('out', 17), ('now', 16), ('economy', 16), ('Americans', 15), ('from', 15), ('better', 15), ('need', 15), ('every', 15), ('over', 15), ('has', 14), ('how', 14), ('so', 14), ('see', 14), ('what', 13), ('like', 13), ('way', 13), ('but', 13), ('than', 13), ('them', 13), ('get', 12), ('’re', 12), ('right', 12), ('future', 12), ('country', 12), ('job', 12), ('some', 11), ('at', 11), ('about', 11), ('most', 11), ('only', 11), ('The', 11), ('he', 11), ('going', 10), ('back', 10), (\"n't\", 10), ('next', 10), ('does', 10)]\n",
      "Trump: [(' ', 347), ('the', 258), ('and', 173), ('to', 157), ('\\n\\n', 155), ('of', 141), ('in', 102), ('Applause', 96), ('our', 89), ('a', 84), ('we', 80), ('is', 65), ('that', 63), ('for', 60), ('have', 47), ('I', 44), ('you', 42), ('are', 42), ('’s', 42), ('We', 35), ('American', 34), ('on', 34), ('will', 31), ('with', 30), ('this', 28), ('years', 28), ('America', 26), ('from', 25), ('very', 25), ('be', 25), ('more', 24), ('Thank', 23), ('it', 22), ('by', 22), ('than', 22), ('not', 21), ('one', 21), ('was', 21), ('And', 20), ('new', 19), ('can', 19), ('must', 19), ('has', 19), ('as', 18), ('United', 17), ('States', 17), ('Americans', 17), ('country', 17), ('us', 16), ('or', 16), ('time', 16), ('they', 16), ('at', 15), ('This', 15), ('In', 15), ('their', 15), ('Congress', 14), ('all', 14), ('an', 14), ('also', 14), ('who', 14), ('tonight', 13), ('now', 13), ('The', 13), ('do', 13), ('USA', 13), ('he', 13), ('border', 13), ('my', 12), ('nation', 12), ('into', 12), ('last', 12), ('women', 12), ('work', 11), ('great', 11), ('It', 11), ('But', 11), ('decades', 11), ('They', 11), ('never', 11), ('much', 11), ('come', 11), ('people', 10), ('jobs', 10), ('administration', 10), ('been', 10), ('n’t', 10), ('Alice', 10), ('illegal', 10), ('two', 9), ('year', 9), ('just', 9), ('most', 9), ('Our', 9), ('ago', 9), ('were', 9), ('applause', 8), ('As', 8), ('together', 8), ('world', 8)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Utility function to calculate how frequently words appear in the text.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)\n",
    "    \n",
    "# The most frequent words:\n",
    "obama_freq = word_frequencies(obama_doc).most_common(100)\n",
    "trump_freq = word_frequencies(trump_doc).most_common(100)\n",
    "print('Obama:', obama_freq)\n",
    "print('Trump:', trump_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama: [('\\n\\n', 86), ('Applause', 84), ('I', 69), ('’s', 68), ('And', 51), ('It', 37), ('n’t', 34), ('America', 33), ('We', 31), ('But', 26), ('people', 26), ('That', 26), ('world', 23), ('work', 22), (\"'s\", 22), ('American', 22), ('’ve', 21), ('want', 19), ('years', 18), ('change', 18), ('new', 18), ('year', 17), ('economy', 16), ('Americans', 15), ('better', 15), ('need', 15), ('like', 13), ('way', 13), ('’re', 12), ('right', 12), ('future', 12), ('country', 12), ('job', 12), ('The', 11), ('going', 10), (\"n't\", 10), ('know', 9), ('believe', 9), ('system', 9), ('time', 9), ('workers', 9), ('past', 9), ('Our', 9), ('security', 9), ('politics', 9), ('lot', 9), ('ISIL', 9), ('families', 8), ('jobs', 8), ('vote', 8), ('’m', 7), ('got', 7), ('big', 7), ('care', 7), ('energy', 7), ('best', 7), ('good', 7), ('’ll', 7), ('power', 7), ('leadership', 7), ('So', 6), ('thing', 6), ('opportunity', 6), ('cut', 6), ('Now', 6), ('long', 6), ('everybody', 6), ('agree', 6), ('sure', 6), ('help', 6), ('voices', 6), ('President', 5), ('Congress', 5), ('fellow', 5), ('come', 5), ('Laughter', 5), ('applause', 5), ('tonight', 5), ('progress', 5), ('needs', 5), ('kids', 5), ('matter', 5), ('planet', 5), ('place', 5), ('economic', 5), ('nation', 5), ('spirit', 5), ('In', 5), ('seven', 5), ('love', 5), ('especially', 5), ('Let', 5), ('start', 5), ('basic', 5), ('hard', 5), ('For', 5), ('political', 5), ('business', 5), ('This', 5), ('military', 5)]\n",
      "Trump: [(' ', 347), ('\\n\\n', 155), ('Applause', 96), ('I', 44), ('’s', 42), ('We', 35), ('American', 34), ('years', 28), ('America', 26), ('Thank', 23), ('And', 20), ('new', 19), ('United', 17), ('States', 17), ('Americans', 17), ('country', 17), ('time', 16), ('This', 15), ('In', 15), ('Congress', 14), ('tonight', 13), ('The', 13), ('USA', 13), ('border', 13), ('nation', 12), ('women', 12), ('work', 11), ('great', 11), ('It', 11), ('But', 11), ('decades', 11), ('They', 11), ('come', 11), ('people', 10), ('jobs', 10), ('administration', 10), ('n’t', 10), ('Alice', 10), ('illegal', 10), ('year', 9), ('Our', 9), ('ago', 9), ('applause', 8), ('As', 8), ('world', 8), ('Now', 8), ('pass', 8), ('THE', 7), ('PRESIDENT', 7), ('fight', 7), ('legislation', 7), ('law', 7), ('know', 7), ('To', 7), ('$', 7), ('President', 6), ('historic', 6), ('citizens', 6), ('parties', 6), ('trade', 6), ('incredible', 6), ('Herman', 6), ('brave', 6), ('build', 6), ('choose', 6), ('progress', 6), ('Tonight', 6), ('like', 6), ('working', 6), ('family', 6), ('AUDIENCE', 6), ('going', 6), ('home', 6), ('prison', 6), ('When', 6), ('deal', 6), ('southern', 6), ('asking', 6), ('Elvin', 6), ('wall', 6), ('Grace', 6), ('Judah', 6), ('agenda', 5), ('defend', 5), ('drugs', 5), ('immigration', 5), ('secure', 5), ('young', 5), ('men', 5), ('heroes', 5), ('million', 5), ('century', 5), ('joined', 5), ('freedom', 5), ('class', 5), ('life', 5), ('confront', 5), ('economic', 5), ('possible', 5), ('economy', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Use our optional keyword argument to remove stop words.\n",
    "obama_freq = word_frequencies(obama_doc, include_stop=False).most_common(100)\n",
    "trump_freq = word_frequencies(trump_doc, include_stop=False).most_common(100)\n",
    "print('Obama:', obama_freq)\n",
    "print('Trump:', trump_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Obama: {'everybody', 'lot', 'job', 'ISIL', 'security', 'way', 'thing', 'long', 'politics', 'future', 'love', 'workers', '’re', 'big', 'hard', 'believe', 'For', 'opportunity', 'matter', 'energy', 'especially', 'spirit', 'good', 'change', 'sure', '’m', 'That', 'families', 'leadership', 'Let', 'fellow', 'place', \"'s\", 'start', '’ve', 'vote', 'help', 'military', 'care', 'best', '’ll', 'business', 'So', 'past', 'system', \"n't\", 'got', 'cut', 'better', 'needs', 'planet', 'power', 'seven', 'basic', 'want', 'political', 'right', 'Laughter', 'kids', 'need', 'voices', 'agree'}\n",
      "Unique to Trump: {'drugs', 'trade', 'Elvin', 'southern', ' ', 'working', 'United', 'agenda', 'USA', 'possible', 'As', 'administration', 'Grace', 'class', 'life', 'Herman', 'million', 'freedom', 'Judah', 'young', 'Thank', 'illegal', 'ago', 'PRESIDENT', 'border', 'choose', 'They', 'law', 'When', 'secure', 'home', 'build', 'AUDIENCE', 'brave', 'decades', 'heroes', 'Alice', 'fight', 'To', 'immigration', 'historic', 'great', '$', 'parties', 'deal', 'women', 'joined', 'States', 'asking', 'wall', 'century', 'pass', 'THE', 'defend', 'confront', 'men', 'prison', 'incredible', 'citizens', 'Tonight', 'family', 'legislation'}\n"
     ]
    }
   ],
   "source": [
    "# Pull out just the text from our frequency lists.\n",
    "obama_common = [pair[0] for pair in obama_freq]\n",
    "trump_common = [pair[0] for pair in trump_freq]\n",
    "\n",
    "# Use sets to find the unique values in each top ten.\n",
    "print('Unique to Obama:', set(obama_common) - set(trump_common))\n",
    "print('Unique to Trump:', set(trump_common) - set(obama_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've just looked at whether certain words are present and how frequently they appear. We can process these words further to remove a little more noise from our data. Consider the words \"think\", \"thought\", and \"thinking\". They're related. They all share the same root word: the verb \"think\". Sometimes we want to focus on the fact that the act of thinking comes up a lot in data, and not have that information split across all the different forms of \"think\".\n",
    "\n",
    "To focus in like this, we can reduce each word to its root, or lemma, and do our counts again. This time we're building a count of concepts rather than just words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Obama: [('-PRON-', 155), ('applause', 89), ('\\n\\n', 86), ('’', 54), ('and', 51), ('be', 45), ('not', 44), ('year', 35), ('america', 33), ('work', 30)]\n",
      "Trump: [(' ', 347), ('\\n\\n', 155), ('-PRON-', 127), ('applause', 104), ('year', 38), ('american', 34), ('thank', 31), ('’s', 28), ('america', 26), ('new', 22)]\n",
      "Unique to Obama: {'not', 'be', 'work', '’', 'and'}\n",
      "Unique to Trump: {'new', ' ', 'american', 'thank', '’s'}\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently lemas appear in the text.\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate our list of most common lemmas.\n",
    "obama_lemma_freq = lemma_frequencies(obama_doc, include_stop=False).most_common(10)\n",
    "trump_lemma_freq = lemma_frequencies(trump_doc, include_stop=False).most_common(10)\n",
    "print('\\nObama:', obama_lemma_freq)\n",
    "print('Trump:', trump_lemma_freq)\n",
    "\n",
    "# Again, identify the lemmas common to one text but not the other.\n",
    "obama_lemma_common = [pair[0] for pair in obama_lemma_freq]\n",
    "trump_lemma_common = [pair[0] for pair in trump_lemma_freq]\n",
    "print('Unique to Obama:', set(obama_lemma_common) - set(trump_lemma_common))\n",
    "print('Unique to Trump:', set(trump_lemma_common) - set(obama_lemma_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
